{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8412673,
          "sourceType": "datasetVersion",
          "datasetId": 5007132
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Librabries"
      ],
      "metadata": {
        "id": "t09gKqDojteA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "! pip install sentence_transformers==2.2.2\n",
        "! pip install -qq -U langchain\n",
        "! pip install -qq -U tiktoken\n",
        "! pip install -qq -U pypdf\n",
        "! pip install -qq -U faiss-gpu\n",
        "! pip install -qq -U InstructorEmbedding\n",
        "! pip install -qq -U transformers\n",
        "! pip install -qq -U accelerate\n",
        "! pip install -qq -U bitsandbytes\n",
        "clear_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05L9dfLv_l-5",
        "outputId": "da8bf052-1e15-43b8-f5d2-9691075af08b",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:08:40.908031Z",
          "iopub.execute_input": "2024-05-14T16:08:40.908400Z",
          "iopub.status.idle": "2024-05-14T16:11:04.362186Z",
          "shell.execute_reply.started": "2024-05-14T16:08:40.908370Z",
          "shell.execute_reply": "2024-05-14T16:11:04.361172Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.35 s, sys: 226 ms, total: 1.57 s\n",
            "Wall time: 2min 59s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXYNYdrctW0c",
        "outputId": "60decf46-36d5-4206-986c-7e6d98485b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import textwrap\n",
        "import time\n",
        "\n",
        "import langchain\n",
        "\n",
        "### loaders\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "\n",
        "### splits\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "### prompts\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "### vector stores\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "### models\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "### retrievers\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0QKUFYF_mxx",
        "outputId": "727551ee-e9dc-4219-846d-27cc08385963",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:11:04.364229Z",
          "iopub.execute_input": "2024-05-14T16:11:04.364554Z",
          "iopub.status.idle": "2024-05-14T16:11:24.199703Z",
          "shell.execute_reply.started": "2024-05-14T16:11:04.364524Z",
          "shell.execute_reply": "2024-05-14T16:11:24.198724Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.4 s, sys: 1.62 s, total: 12.1 s\n",
            "Wall time: 18.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('langchain:', langchain.__version__)\n",
        "print('torch:', torch.__version__)\n",
        "print('transformers:', transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFVg-2wcAGM7",
        "outputId": "daa0ae5d-5168-4c33-8b4b-ca3f19491ce8",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:11:24.200827Z",
          "iopub.execute_input": "2024-05-14T16:11:24.201372Z",
          "iopub.status.idle": "2024-05-14T16:11:24.206566Z",
          "shell.execute_reply.started": "2024-05-14T16:11:24.201343Z",
          "shell.execute_reply": "2024-05-14T16:11:24.205698Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain: 0.1.20\n",
            "torch: 2.2.1+cu121\n",
            "transformers: 4.40.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration: The code sets up a configuration class CFG with various parameters such as the model name, temperature, top-p, repetition penalty, chunk size for splitting text, embeddings model repository, number of similar passages to retrieve, and paths."
      ],
      "metadata": {
        "id": "14VZIPBAkvux"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zO6n8Vqp1EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jvV_2aZ5p1ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gNHQ0aG9p1Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # LLMs\n",
        "    model_name = 'llama2-7b-chat'\n",
        "    temperature = 0\n",
        "    top_p = 0.95\n",
        "    repetition_penalty = 1.15\n",
        "    # splitting\n",
        "    split_chunk_size = 800\n",
        "    split_overlap = 0\n",
        "    # embeddings\n",
        "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "    # similar passages\n",
        "    k = 3\n",
        "\n",
        "    PDFs_path = '/content/NLP_dataset.pdf'\n"
      ],
      "metadata": {
        "id": "k6EH5QU1B4Vo",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:30:06.272454Z",
          "iopub.execute_input": "2024-05-14T16:30:06.272874Z",
          "iopub.status.idle": "2024-05-14T16:30:06.278266Z",
          "shell.execute_reply.started": "2024-05-14T16:30:06.272825Z",
          "shell.execute_reply": "2024-05-14T16:30:06.277264Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Initialization: The get_model function initializes a language model and tokenizer based on the specified model_name in the configuration. It downloads the specified model and tokenizer from the Hugging Face model hub. The BitsAndBytesConfig is used for efficient quantization of model parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "O_pGftkYlJPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model = CFG.model_name):\n",
        "\n",
        "    print('\\nDownloading model: ', model, '\\n\\n')\n",
        "\n",
        "    if model == 'wizardlm':\n",
        "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True\n",
        "        )\n",
        "\n",
        "        max_len = 1024\n",
        "\n",
        "    elif model == 'llama2-7b-chat':\n",
        "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "            trust_remote_code = True\n",
        "        )\n",
        "\n",
        "        max_len = 2048\n",
        "\n",
        "    else:\n",
        "        print(\"Not implemented model (tokenizer and backbone)\")\n",
        "\n",
        "    return tokenizer, model, max_len"
      ],
      "metadata": {
        "id": "GBq3yS_FCRMe",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:30:07.794112Z",
          "iopub.execute_input": "2024-05-14T16:30:07.794504Z",
          "iopub.status.idle": "2024-05-14T16:30:07.806615Z",
          "shell.execute_reply.started": "2024-05-14T16:30:07.794473Z",
          "shell.execute_reply": "2024-05-14T16:30:07.805718Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tokenizer, model, max_len = get_model(model = CFG.model_name)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "NC_LHFlfCdSY",
        "outputId": "9b21ddb1-e21a-485d-b83b-7dd587179915",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:30:08.980990Z",
          "iopub.execute_input": "2024-05-14T16:30:08.981615Z",
          "iopub.status.idle": "2024-05-14T16:31:38.316240Z",
          "shell.execute_reply.started": "2024-05-14T16:30:08.981584Z",
          "shell.execute_reply": "2024-05-14T16:31:38.315400Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading model:  llama2-7b-chat \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2b3ead908e2f>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mmodel_repo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mquantization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3165\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3166\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m\"and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "js4lTg85CoPD",
        "outputId": "e185a1ca-524f-44a3-c62c-eb8dd03ebfaf",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:31:38.317919Z",
          "iopub.execute_input": "2024-05-14T16:31:38.318196Z",
          "iopub.status.idle": "2024-05-14T16:31:38.328918Z",
          "shell.execute_reply.started": "2024-05-14T16:31:38.318170Z",
          "shell.execute_reply": "2024-05-14T16:31:38.328115Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-67f96fad7f04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.hf_device_map"
      ],
      "metadata": {
        "id": "VaICU4x1CrjE",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:31:38.329840Z",
          "iopub.execute_input": "2024-05-14T16:31:38.330099Z",
          "iopub.status.idle": "2024-05-14T16:31:41.219411Z",
          "shell.execute_reply.started": "2024-05-14T16:31:38.330077Z",
          "shell.execute_reply": "2024-05-14T16:31:41.218312Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Hugging Face Pipeline: The code sets up a pipeline for text generation using the Hugging Face pipeline function. It utilizes the pre-trained model and tokenizer obtained earlier. Various parameters such as max_length, temperature, top_p, and repetition_penalty are configured for text generation."
      ],
      "metadata": {
        "id": "QLyIzRo8lOhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### hugging face pipeline\n",
        "pipe = pipeline(\n",
        "    task = \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "#     do_sample = True,\n",
        "    max_length = max_len,\n",
        "    temperature = CFG.temperature,\n",
        "    top_p = CFG.top_p,\n",
        "    repetition_penalty = CFG.repetition_penalty\n",
        ")\n",
        "\n",
        "### langchain pipeline\n",
        "llm = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "id": "S_GWOy5yDdZy",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:31:41.221348Z",
          "iopub.execute_input": "2024-05-14T16:31:41.221651Z",
          "iopub.status.idle": "2024-05-14T16:31:41.229915Z",
          "shell.execute_reply.started": "2024-05-14T16:31:41.221625Z",
          "shell.execute_reply": "2024-05-14T16:31:41.229116Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng3HyPmdD6Fo",
        "outputId": "fd16ea82-ecb7-452c-d975-0b6fa1b4728c",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:31:41.230949Z",
          "iopub.execute_input": "2024-05-14T16:31:41.231245Z",
          "iopub.status.idle": "2024-05-14T16:31:41.244895Z",
          "shell.execute_reply.started": "2024-05-14T16:31:41.231218Z",
          "shell.execute_reply": "2024-05-14T16:31:41.244141Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7cb20f38e6b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Bt9ndhmI9vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"Chennai places\"\n",
        "llm.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "D8U4M4oVD99U",
        "outputId": "20f980d0-8ef8-4113-984e-8ea77077a7f3",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:31:54.670890Z",
          "iopub.execute_input": "2024-05-14T16:31:54.671263Z",
          "iopub.status.idle": "2024-05-14T16:32:45.985299Z",
          "shell.execute_reply.started": "2024-05-14T16:31:54.671220Z",
          "shell.execute_reply": "2024-05-14T16:32:45.984409Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 56.8 s, sys: 211 ms, total: 57 s\n",
            "Wall time: 1min 4s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Chennai places to visit, Chennai sightseeing, Chennai tourism, Chennai travel guide, Chennai trip planner, India holidays, Indian cities, South IndiaLeave a comment on Chennai: A City of Rich History and Culture\\n\\nChennai, the capital city of Tamil Nadu, is a vibrant metropolis that offers a unique blend of traditional culture and modern amenities. Located on the southeastern coast of India, Chennai has a rich history dating back to the 16th century when it was a major port city for the British East India Company. Today, it is one of the largest cities in India and an important cultural hub in South Asia.\\nOne of the most popular attractions in Chennai is the Kapaleeswarar Temple, a beautiful Shiva temple built in the 7th century AD. The temple is famous for its intricate carvings and sculptures, as well as its towering gopuram (gateway) that rises over 40 feet high. Another must-visit attraction is the Fort St. George Museum, which houses a vast collection of artifacts and exhibits related to the city's colonial past.\\nFor those interested in art and culture, Chennai has plenty to offer. The city is home to several museums, including the National Art Gallery, which showcases a wide range of contemporary and traditional Indian art. The Madras Music Academy is another institution worth visiting, as it hosts concerts and performances throughout the year featuring local and international artists.\\nWhen it comes to food, Chennai is known for its spicy cuisine, which reflects the city's Tamil heritage. Some popular dishes include dosas (fermented rice and lentil crepes), idlis (steamed rice cakes), and sambars (spicy vegetable stews). For a truly authentic experience, be sure to try some of the street food stalls located throughout the city.\\nIn addition to its cultural attractions, Chennai also offers a variety of outdoor activities. The Elliot's Beach is a popular spot for swimming and sunbathing, while the Guindy National Park provides a peaceful escape from the hustle and bustle of the city. The Vivekananda House, a historic building where Swami Vivekananda stayed during his visit to Chennai in 1897, is now a museum dedicated to the life and teachings of the great Hindu philosopher.\\nOverall, Chennai is a fascinating city that offers something for everyone. Whether you are interested in history, culture, food, or outdoor activities, there is no shortage of things to see and do in this vibrant metropolis. So why not plan your next trip to Chennai and discover all that this incredible city has to offer?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "query = \"Plan a trip to goa for 1 day\"\n",
        "llm.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "OUI6RJ_JD_Vm",
        "outputId": "058d9e4a-2de5-4b4a-9251-8e0db420bc6e",
        "execution": {
          "iopub.status.busy": "2024-05-14T16:32:45.987029Z",
          "iopub.execute_input": "2024-05-14T16:32:45.987323Z",
          "iopub.status.idle": "2024-05-14T16:33:29.366427Z",
          "shell.execute_reply.started": "2024-05-14T16:32:45.987298Z",
          "shell.execute_reply": "2024-05-14T16:33:29.365463Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 41.4 s, sys: 0 ns, total: 41.4 s\n",
            "Wall time: 41.6 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Plan a trip to goa for 1 day. Unterscheidung between the two is not always clear-cut, and different people may have different preferences when it comes to which type of vacation they want to take.\\nThe best way to plan a trip to Goa depends on your personal preferences and interests. Here are some steps you can follow:\\n1. Determine the purpose of your trip: Are you looking for a relaxing beach vacation or an adventure-filled getaway? Knowing the purpose of your trip will help you decide where to stay, what activities to do, and how long to stay in Goa.\\n2. Choose your travel dates: Goa has a tropical climate, so the best time to visit is from October to May. Avoid planning your trip during the monsoon season (June to September) as the weather can be unpredictable and many tourist attractions may be closed.\\n3. Decide on your accommodation: Goa offers a wide range of accommodations, from budget-friendly guesthouses to luxury resorts. Consider factors like location, amenities, and price when choosing where to stay.\\n4. Plan your itinerary: Once you know your travel dates and accommodation, start planning your itinerary. Some popular things to do in Goa include visiting beaches like Palolem, Vagator, and Anjuna; exploring old Goa towns like Panaji and Margao; and taking part in water sports like snorkeling and kayaking.\\n5. Book flights and transportation: Look for affordable flight options to Goa's nearest airport, Dabolim Airport. From there, you can take a taxi or bus to your final destination.\\n6. Pack accordingly: Make sure to pack lightweight and breathable clothing, sunscreen, sunglasses, and comfortable shoes for walking around. If you plan to participate in water sports, don't forget to bring your swimwear!\\n7. Research local culture: Goa has a rich cultural heritage, with influences from both Indian and Portuguese traditions. Learn about the local customs, food, and music to make the most out of your trip.\\n8. Stay safe: Always prioritize safety while traveling. Be aware of your surroundings, keep valuables secure, and avoid traveling alone at night.\\nBy following these steps, you can create a memorable and enjoyable trip to Goa that meets your needs and expectations.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4aZBtpKZ1NR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}